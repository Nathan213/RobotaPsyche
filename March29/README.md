# Reading Inspired Discussion

## How qualified are non-robot designers to talk about the ethics of robots?
At this point, we have already discussed the difference between the appearence and funcationality of robots. As amatuer robot designer ourselves, I trust we have a vague idea of how to trick people into empathizing with a machine, even though we know the machine is just following code. Steinert crucially said in his text that "ethicists working on the ethical significance of robots should consider the possible disadvantages of adjusting their paradigms and concepts in order to accommodate the latest technical developments." But without truly understanding the functionality behind a robot, is it ok to ethically judge it?

## What are some possible “gravitational centers” for the ethics of robotically enhanced humans?
A common topic in the readings is the neccesity of highly autonomous robots, if we can robotically enhance ourselves. Yet, none of the readings discuss the ethics of robotically enhanced humans. If we reach a level of technological prowess where humans can be enhanced drastically from what we are now, there are countless ethical questions that will arise. For one, I trust it is quite predictable that not all humans will have access to the same level of enhancements. What implications might this have on social and race theory?

## Is the “good life” and innovation positively correlated?
Steinert brings up Aristotle's concept of "the good life". How are we _supposed_ to live? Up to now, the quest for a good life has mostly been improved by innovation. From agriculture to industrial revolution to AI. Is there a point in the future where the “good life” is achieved by stopping innovation? Aka. can innovation become harmful to the good life one day?

## Can we truly classify advanced robots as a race?
From my limited understanding of races and evolution, there are big categories of organisms that are organized through multiple layers, until a group of sufficiently similar organisms are defined as a race. In all the papers we have read, one of the constant concerns is how do we classify robots? Should we consider them as humans, as animals, or what? My question is, can we truly, and do we need to classify them like an organism-like being? Imagine the impossibility of doing so. How do we differentiate a Roomba from a waitress robot? How do we differentiate between an old model of Roomba and a new model? They are all robots: (1) if we see them as one, there are clear challenges to classifying them as a type of organism (2) if we don’t see them as one race, the sheer variety that robots can be makes it near impossible to define them in terms of a race anyways.

## Do Robots even need rights from us?
So much of the deabte is the rights of advanced robots of the future. Yet, again and again, I feel a strong sense of human centric tendencies in this discussion. This reminds me of the great debate between Las Casas and Sepulveda. Their debate concerned the treatment of Indians, whether they were barbarians, fellow humans, or even objects. While Las Casas saw them as fellow humans and strongly opposed their slavery, Sepulveda saw it as fitting. In his texts, Sepulveda argued that the Indians were barbarians who did not know law, manners, or even Christianity. This makes them fitting to be dominated by European powers. Pulling the discussion back to robots, I am constantly afraid that we are viewing advanced robots with a scope similar to Sepulveda. He viewed Indians through the lens of Christianity and the laws of his land. My question is, is it even fair for us to discuss whether we should give robots rights or not? Are our rights even fitting for them as an entity? For example, most of modern civilization highly value individual privacy, both digitally and in regular daily life. It's not hard to imagine future robots viewing such rights to privacy as futile. (Eg. everyone stores everything in a cloud). So why should they even care about the rights we intend or not intend to give them?
